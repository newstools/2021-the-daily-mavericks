(Image: Adobe Stock) Eleonore Pauwels is a senior research fellow at the Global Center on Cooperative Security and is an expert on the convergence of AI and other dual use technologies. She is the author of the volume The Anatomy of Information Disorders in Africa published by the Konrad-Adenauer Foundation. Karen Allen is a research adviser on emerging threats in Africa for the Institute for Security Studies and has published work on biometric surveillance and digital vigilantism in Africa. She is based in South Africa and is a former foreign correspondent with the BBC. Read and watch Daily Maverick’s webinar on this topic here. Across countries in sub-Saharan Africa, the proliferation of data capture, biometrics and surveillance technologies, driven by economic and national security interests, poses new threats to populations. With general elections on the horizon in countries including Ethiopia and Zambia, the need to come up with safeguards to “tame the tech” could not be more pressing. As algorithms become more sophisticated and powerful, and are able to shape as well as capture human behaviour, private sector and civil society organisations must learn and design “ethical AI” to prevent and mitigate collective data harms. Scores of African countries, Kenya and South Africa among them, are in the process of registering their populations’ biometrics and sensitive information into centralised databases. The justification often used is that it improves efficiency and ensures wider access to services. The Huduma Namba system in Kenya, which has already been challenged on legal grounds, is one such example. Yet such new forms of monitoring and controlling populations increasingly serves a securitisation agenda. AI combined with biometrics and facial recognition technology is already helping the leadership of illiberal regimes impose surveillance and repression. Additionally, with the export of digital products such as surveillance cameras and sophisticated software, there is the prospect of surveillance norms being exported, too. The potential result is that given its dominance in the technology market, China’s model of algorithmic surveillance could be used to deploy AI-led ethnic profiling in societies that are experiencing ethnic violence. There are also fears that in countries such as Uganda, the technology could be used to single out members of the LGBTQI+ community. These concerns spill over beyond the world of surveillance and impact on democracies in which elections and institutions of law and order are at their core. In African countries where privacy and data-protection policies are not translated into robust regulations or safeguards, controlling populations’ “digital bodies and minds” – literally manipulating groups’ behaviours and conversations – can be used for disrupting elections, undermining the rule of law and securing illiberal state power. The state and private sector players involved in Kenyan and Nigerian elections have already exploited the combination of AI and populations’ sensitive data to exert political and social engineering, eroding citizens’ political agency and trust in governing institutions. Other examples of technologies that make bodies and minds increasingly traceable include the AI software called “iSentry” used to detect “abnormal behaviour” on the streets of Johannesburg. Mobile biometric devices have been deployed by the Ugandan Police Force that use AI to confirm a person’s identity on the spot by instantly matching it against a database. Also, facial and behaviour recognition tools deployed in Zimbabwe have been used to predict the movements and actions of individuals in public spaces. The coming together of AI and large-scale data collection potential not only enables the documentation and profiling of people as they live, move and feel. But the lack of safeguards has enabled data analytics firms to make a business out of identifying individuals’ deepest fears, hatreds and prejudices and manipulating subliminal cues to mobilise them around divisive issues, or to induce voting distrust and apathy. African societies have now entered this “Internet of Bodies and Minds”, where the patterns of daily lives – composed of digital, physical and biological data – have become free material for behavioural surveillance. A trend already being observed is the increasing capacity and willingness of some governments to exploit digital networks for inflaming existing racial, social and economic divisions between subpopulations. In many African countries, where privacy and data protection policies are not translated into robust regulations, state and private sector actors can extract sensitive personal data from an array of online population databases for targeting ethnic and socioeconomic groups. These platforms can provide precise insights into a person’s ethnic background, political affiliation, education level, wealth, location and cellphone number. For instance, Kenyan expert Robert Muthuri explains how, in 2017 – prior to the election – voters’ demographic information was connected by political parties to social media profiles and cellphone numbers to achieve more precise targeting within personal networks. A second trend concerns the deployment of AI-enabled ethnic profiling in countries that are still suffering from violence related to ethnic divisions. Relying on the aggressive campaigns produced by PR companies like Cambridge Analytica or the SCL Group, domestic political parties can exploit citizens’ personal profiles and information networks for spreading rumours, targeted propaganda, hate speech, mis- and disinformation. For instance, political campaigns in Kenya in 2017 and in Nigeria in 2015 relied on video propaganda that built on ethnic and socioeconomic tensions to target segments of the electorate defined by ethnicity, political leanings and age. The rationale behind such sophisticated disinformation architecture is to immerse citizens in an alternative, virtual reality where they themselves become producers of emotional manipulation. A third trend has witnessed the tools of behavioural surveillance being used to also silence civil society and media groups that may offer resistance. And so rising concerns for populations are new forms of closing the “virtual civic space”. Fourth, the political consultancy firms appear to be using their digital tools as part of a current race for strategic positioning in Africa and the longer-term objective of colluding with states to shape a particular electoral outcome. Some of these foreign companies are promised access to growing markets and industries involving data, oil, genetic and biodiversity resources, rare-earth minerals and metals. With elections on the horizon in Ethiopia, Zambia, Gambia and Libya, to name but a few, the need to keep “tech in check” and ensure human safeguards remain at the core couldn’t be more pressing. There is an urgent need to devise a culture of do-no-harm in the AI and biometrics (data-capture) space. The cultivation of such a collective norm would aim to prevent the deployment of AI, biometrics and behavioural surveillance technologies in contexts where there are inadequate safeguards to protect human rights and insufficient mechanisms to ensure accountability. Such tools may include human impact assessments to weigh the benefits of harnessing data-capture technologies against the costs to civilian security and human rights. They may also include mechanisms to audit algorithms and to monitor new technological capabilities as they emerge. Without such robust accountability mechanisms, Africa could become a testing ground for unbridled AI and a theatre for a new digital arms race to secure influence over people and resources. DM Please note you must be a Maverick Insider to comment. Sign up here or sign in if you are already an Insider. Everybody has an opinion but not everyone has the knowledge and the experience to contribute meaningfully to a discussion. That’s what we want from our members. Help us learn with your expertise and insights on articles that we publish. We encourage different, respectful viewpoints to further our understanding of the world. View our comments policy here. No Comments, yet