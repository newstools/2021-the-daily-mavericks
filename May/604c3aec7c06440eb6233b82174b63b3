Left: Executive director of the University of California, Berkeley Human Rights Center and a lecturer at the university’s School of Law, Dr Alexa Koenig | Former BBC Foreign Correspondent Karen Allen. (Photos: Supplied) Using artificial intelligence (AI) to distort audio and video, convincing deepfakes can be made by pretty much anyone with the right hardware and software and a few hours to kill. The results can wreak havoc on individual livelihoods and reputations, but more frighteningly, can be used to manipulate en masse. While this may sound like the finishing touches on a looming dystopia, there’s hope for us all yet.  Around 500 hours of video content are uploaded to the social sharing site YouTube every minute of every day. The site consequently houses an unimaginable amount of video — and is just one of many that allow anyone anywhere to upload moving images of themselves and others, with little to no restrictions or oversight. As Dr Alexa Koenig of the University of Berkeley Human Rights Center pointed out in conversation with the Institute for Security Studies’ senior research adviser, Karen Allen, it’s impossible to know how much of that endless ream of content is real and how much is fake. Manipulating content — text, documents, photographs, images and even audio for malevolent or political purposes — is as old a propaganda technique as telling porky pies. But until digital technology gave us the tools to distort video, it was tacitly accepted that moving images with sound were a reliable form of evidence. That has changed, thanks to deepfakes. “Deepfake videos are what’s often referred to as synthetic video, said Koenig. “They’re events that are generated by algorithms, so essentially by two computers that are challenging each other to see how well they can trick human beings into thinking that what they are creating is a depiction of reality,” she said. Take this deepfake that seems, to all intents and purposes, to depict Tom Cruise behaving like, well, Tom Cruise. The videos and the person in them are real. The depiction of an unnervingly weird celebrity is not. More than anything, the threat posed by deepfake technology comes down to intent, said Koenig. There are applications for manipulating sound and video that are positive: rendering deceased celebrities like Carrie Fisher as not only alive, but impressively lifelike in some of the world’s favourite film franchises. Distorting the ages of actors so they look older or younger, without resorting to expensive post-production techniques. Creating useful and lifelike training scenarios for many different industries, using virtual reality. It’s when the intention is malevolent — and perpetrated by bad actors with endless resources and appalling human rights records (we’re looking at you, Russia and China) — that the use of this technology becomes deeply problematic and threatening. “It’s becoming much more accessible and more sophisticated,” said Koenig. “For around $35, you can buy basic technology to make these deepfakes for your own uses. But it’s only a matter of time until it’s way more pervasive. There are reasons why people would want to [mass] manipulate information for political goals. We’re not quite there yet with this technology, but we need to plan for the future,” she said. “This is the arms race that we’re seeing today.” Kovelin Naidoo, the group cyber risk officer for one of South Africa’s largest banks, leads a team that tests cyber resiliency by identifying organised cybercrime groups and modelling possible or emerging threats. In other words, part of his work entails making deepfakes — but for risk research purposes. According to Naidoo, a large proportion of people are exposed to, and are even using the building blocks for, deepfake technology on a regular basis. Smartphones are increasingly making facial and object recognition part of their interfaces (go into the Photos app on your iPhone and search for “birthday cake” or “waterfall”). Apps like Instagram and TikTok offer filters that change and distort your selfies in cute but also terrifying ways. The commercial applications for neural networks and machine learning — the building blocks of artificial intelligence — are myriad, and already ubiquitous. “For a motivated threat-actor or hacker with a lot more time, computing power and props? This becomes more threatening,” said Naidoo, who also explained that it’s much harder to police in “out-of-band” spaces like WhatsApp, where end-to-end encryption means platforms and companies aren’t directly monitoring usage. The good news is that, as the sophistication of deepfake technology develops, so too do the tools and resources that can be used to monitor and regulate its use, along with the number of smart people and organisations actively working to combat it. “We’re in a technology cycle,” said Naidoo. “This tech is new and it’s going to evolve over time, but we will get better at it, and in time detection mechanisms will be built in.” Detecting deepfakes So what are the clues ordinary consumers of content can use to detect deepfakes and warn others about them? First, Koenig says, check the metadata — the information inherent in any piece of content — that might tell you details about its provenance. Does it line up with the messaging or the details being imparted? Is there consistency between image or audio and information? Second, check your source. Did you receive this content from someone you know and trust to be discerning, or view it on a trusted news website? Or has it been forwarded many times from an unknown origin? Third, look for “tells”. Deepfake tech dates very quickly and isn’t yet sophisticated enough to get every single human trait 100% perfect. Is the person depicted blinking and breathing normally, for example? Or does something about their bearing and mannerisms feel off? If it does, you’re likely looking at a manipulated image. Finally, if you’re suspicious, check the internet to see if other stories or content exist that confirm what you’re seeing is real. Any photo or video of a public event exists as part of a bigger ecosystem in today’s world, so it’ll often be possible to double check. Organisations that help citizens are beginning to establish themselves, activists and media outlets verify videos and other content, like Witness, and Reality Defender. Increasing pressure is also being brought to bear on social media and tech companies to take deepfakes seriously and develop the means within their platforms to cut malevolent content off at the pass before it even enters the public realm. “One way to shift incentives is to create new laws and higher penalties or disincentives for bad actors,” said Koenig. “Taking a piecemeal approach to legislation is missing what we’re going to have to start grappling with, when information is borderless.” DM Please note you must be a Maverick Insider to comment. Sign up here or sign in if you are already an Insider. Everybody has an opinion but not everyone has the knowledge and the experience to contribute meaningfully to a discussion. That’s what we want from our members. Help us learn with your expertise and insights on articles that we publish. We encourage different, respectful viewpoints to further our understanding of the world. View our comments policy here. No Comments, yet